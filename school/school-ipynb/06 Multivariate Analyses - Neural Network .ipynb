{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c5890-558c-4f33-90fb-eeaf0e99cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore the ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd4350c-5285-4985-b6cd-d1b0f574f3ed",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks \n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook kernel was created to help you understand more about machine learning. I intend to create tutorials with several machine learning algorithms from basic to advanced. I hope I can help you with this data science trail. For any information, you can contact me through the link below.\n",
    "\n",
    "Contact me here: https://www.linkedin.com/in/vitorgamalemos/\n",
    "\n",
    "## Introduction \n",
    "\n",
    "<img src=\"https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs40846-016-0191-3/MediaObjects/40846_2016_191_Fig1_HTML.gif\">\n",
    "\n",
    "<p style=\"text-align: justify;\">Artificial Neural Networks are mathematical models inspired by the human brain, specifically the ability to learn, process, and perform tasks. The Artificial Neural Networks are powerful tools that assist in solving complex problems linked mainly in the area of combinatorial optimization and machine learning. In this context, artificial neural networks have the most varied applications possible, as such models can adapt to the situations presented, ensuring a gradual increase in performance without any human interference. We can say that the Artificial Neural Networks are potent methods can give computers a new possibility, that is, a machine does not get stuck to preprogrammed rules and opens up various options to learn from its own mistakes.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ea60e-cf16-46fa-9335-197f00b53f73",
   "metadata": {},
   "source": [
    "## Biologic Model\n",
    "\n",
    "<img src=\"https://www.neuroskills.com/images/photo-500x500-neuron.png\">\n",
    "<p style=\"text-align: justify;\">Artificial neurons are designed to mimic aspects of their biological counterparts. The neuron is one of the fundamental units that make up the entire brain structure of the central nervous system; such cells are responsible for transmitting information through the electrical potential difference in their membrane. In this context, a biological neuron can be divided as follows.</p>\n",
    "\n",
    "**Dendrites** ‚Äì are thin branches located in the nerve cell. These cells act on receiving nerve input from other parts of our body.\n",
    "\n",
    "**Soma** ‚Äì acts as a summation function. As positive and negative signals (exciting and inhibiting, respectively) arrive in the soma from the dendrites they are added together.\n",
    "\n",
    "**Axon** ‚Äì gets its signal from the summation behavior which occurs inside the soma. It is formed by a single extended filament located throughout the neuron. The axon is responsible for sending nerve impulses to the external environment of a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42de27de-036c-446f-940b-c7ab943cbca5",
   "metadata": {},
   "source": [
    "## Artificial Neuron as Mathematic Notation\n",
    "In general terms, an input X is multiplied by a weight W and added a bias b producing the net activation. \n",
    "<img style=\"max-width:60%;max-height:60%;\" src=\"https://miro.medium.com/max/1290/1*-JtN9TWuoZMz7z9QKbT85A.png\">\n",
    "\n",
    "We can summarize an artificial neuron with the following mathematical expression:\n",
    "$$\n",
    "\\hat{y} = f\\left(\\text{net}\\right)= f\\left(\\vec{w}\\cdot\\vec{x}+b\\right) = f\\left(\\sum_{i=1}^{n}{w_i x_i + b}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073dc3b-0cd9-425c-87ea-aec322435563",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## The Singlelayer Perceptron\n",
    "\n",
    "<p style=\"text-align: justify;\">The Perceptron and its learning algorithm pioneered the research in neurocomputing. the perceptron is an algorithm for supervised learning of binary classifiers [1]. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.<p>\n",
    "    \n",
    "<img src=\"https://www.edureka.co/blog/wp-content/uploads/2017/12/Perceptron-Learning-Algorithm_03.gif\">\n",
    "    \n",
    "#### References\n",
    "    \n",
    "- Freund, Y.; Schapire, R. E. (1999). \"Large margin classification using the perceptron algorithm\" (PDF). Machine Learning\n",
    "\n",
    "- Aizerman, M. A.; Braverman, E. M.; Rozonoer, L. I. (1964). \"Theoretical foundations of the potential function method in pattern recognition learning\". Automation and Remote Control. 25: 821‚Äì837.\n",
    "  \n",
    " \n",
    "- Mohri, Mehryar and Rostamizadeh, Afshin (2013). Perceptron Mistake Bounds.\n",
    "\n",
    "Source: https://www.kaggle.com/code/vitorgamalemos/perceptron-neural-network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9edccc-3bc7-400f-9b6a-96d2c9ac9580",
   "metadata": {},
   "source": [
    "## Training a Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6edd5-0ed0-417d-b05d-47d20e46b489",
   "metadata": {},
   "source": [
    "We are going to use a Perceptron from the `sklearn` library to build a simple classification model. We make use of a MultiLayerPerceptron as the name suggest in default it has multiple layers (100). In this excercise we bring this down to a perceptron which is basically a MLP with only one layer, so the variable 'hidden_layer_sizes' will be 1.  \n",
    "Before training, we must prepare the data by applying Standard Scaling (to normalize the data) and One-Hot Encoding (to convert categorical values into a format suitable for the model).  \n",
    "\n",
    "##### 1. Data Preprocessing\n",
    "- One-Hot Encoding converts categorical features into a numerical format by creating binary columns for each category.\n",
    "- Standard scaling transforms features to have a mean of 0 and a standard deviation of 1. This normalization step is crucial for models like Perceptron, which are sensitive to varying scales in input data. Without scaling, features with larger magnitudes can dominate the learning process, leading to poor model performance.\n",
    "\n",
    "##### 2. Splitting the Data\n",
    "Like with Decision Trees or Random Forests, we split our dataset into training and test sets to evaluate model performance.\n",
    "\n",
    "##### 3. Training the Perceptron\n",
    "We initialize the Perceptron model and train it using gradient descent.  \n",
    "The two key parameters we start fine-tuning with are:\n",
    "\n",
    "###### Learning Rate (`eta0`)\n",
    "- Controls how much the model updates its weights during training.\n",
    "- **High learning rate:** Fast convergence, but may overshoot optimal weights.\n",
    "- **Low learning rate:** More stable, but slow training.\n",
    "\n",
    "###### Epochs (`max_iter`)\n",
    "- Defines the number of passes over the dataset during training.\n",
    "- More epochs allow the model to learn better, but too many can cause overfitting.\n",
    "\n",
    "##### 4. Fine-Tuning the Model\n",
    "After training, we can fine-tune hyperparameters like:\n",
    "- Learning rate (`eta0`)\n",
    "- Epochs (`max_iter`)\n",
    "\n",
    "By adjusting these parameters, we aim to improve accuracy and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "486a13c1-ff7e-45d6-8514-880a88286ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0d6c4b1-138e-4a07-a910-a57e49fcc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15b5b8ca-50a9-4e25-addb-164f0ae378b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data (scale the features)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf9184",
   "metadata": {},
   "source": [
    "üöÄ **Why Scale the Features in Machine Learning?**\n",
    "\n",
    "Scaling features means adjusting the values of your data so that they are within a specific range. The most common method is Standard Scaling, where the values are adjusted to have:\n",
    "\n",
    "A mean of 0.\n",
    "A standard deviation of 1.\n",
    "\n",
    "\n",
    "üîç **Why is This Important?**\n",
    "\n",
    "üöÄ Speed Up Training:\n",
    "Algorithms like Gradient Descent converge faster when features are on a similar scale.\n",
    "Without scaling, features with larger values dominate the learning process.\n",
    "\n",
    "üîß Improve Accuracy:\n",
    "Many algorithms (like Neural Networks, SVMs, K-Means) perform better when features are scaled.\n",
    "\n",
    "If one feature has values like [0, 1, 2] and another like [1000, 2000, 3000], the larger values can overpower the smaller ones.\n",
    "\n",
    "‚öñÔ∏è Maintain Balance:\n",
    "Features with different ranges can make the model focus too much on larger values.\n",
    "\n",
    "**Scaling ensures that all features are treated equally.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5562e30-f5d0-4799-af89-31697f388672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c6f14-0f33-4149-b1f7-12cde9921dc1",
   "metadata": {},
   "source": [
    "#### Try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324b673-ce5e-4547-b1e6-9c63d4471e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5750\n",
      "Test Accuracy: 0.6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marjolein\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set the learning rate, max_iter, and hidden layer sizes as specified\n",
    "learning_rate = 0.01\n",
    "max_iter = 50 # from 5 to 40\n",
    "hidden_layer_sizes = 1\n",
    "\n",
    "# Initialize the MLPClassifier with the given hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for both training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1814f3a-b526-465a-b34d-c6d83b8a253e",
   "metadata": {},
   "source": [
    "#### Try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae8fc00-5031-4908-8510-23828bbd3fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate, max_iter, and hidden layer sizes as specified\n",
    "learning_rate = 0.05 # from 0.001 to 0.1 like 0.01 or 0.05 \n",
    "max_iter = 4 \n",
    "hidden_layer_sizes = 1\n",
    "\n",
    "# Initialize the MLPClassifier with the given hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for both training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897e171-6a9f-4292-9512-52033ef8c8aa",
   "metadata": {},
   "source": [
    "#### Try 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3412fbd-0412-4593-9165-62d27a7c4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate, max_iter, and hidden layer sizes as specified\n",
    "learning_rate = 0.1\n",
    "max_iter = 50\n",
    "hidden_layer_sizes = 1\n",
    "\n",
    "# Initialize the MLPClassifier with the given hyperparameters\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    learning_rate_init=learning_rate,\n",
    "    max_iter=max_iter,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for both training and test sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5178f-d5f6-4113-be4e-9613a3aed107",
   "metadata": {},
   "source": [
    "### Portfolio assignment 20\n",
    "30 min: Train a perceptron to predict the number of the MNIST dataset.\n",
    "- Fit a Perceptron model (keep hidden_layer_sizes=1) using the images in de fetch openml dataset.\n",
    "- Change the learning_rate and max_iter to find the 'right fit'.\n",
    "- Use your perceptron to make predictions for both the train and test set.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a39c355-8f43-4d8b-8302-f6c1e885f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b61c2-aa2f-4b9b-b547-1cd4c4c3d04c",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Fine-tune the learning rate and epochs (max iterations).\n",
    "- Calculate the accuracy for both the train set predictions and test set predictions, what happens per iteration?\n",
    "- Is the accurracy different? Did you expect this difference?\n",
    "\n",
    "Optional: Perform the same tasks but change the hidden_layer_sizes <br>\n",
    "\n",
    "Findings: ...<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5aff5e-b2fa-4e43-8470-9997e3217120",
   "metadata": {},
   "source": [
    "### Portfolio assignment 21\n",
    "30 min: Train a perceptron to predict one of the categorical columns of your own dataset.\n",
    "- Prepare the data:<br>\n",
    "    - <b>Note</b>: Some machine learning algorithms can not handle missing values. You will either need to: \n",
    "         - replace missing values (with the mean or most popular value). For replacing missing values you can use .fillna(\\<value\\>) https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html\n",
    "         - remove rows with missing data.  You can remove rows with missing data with .dropna() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html <br>\n",
    "    - <b>Note</b>: Some machine learning algorithms can not handle categorical values. You will either need to:\n",
    "        -  To handle categorical data, you can use One-Hot Encoding to convert categories into binary columns with .get_dummies(<value(s)>). This creates a new column for each category\n",
    "- Split your dataset into training (70%) and testing (30%) sets. \n",
    "- Use your Perceptron to make predictions for both the train and test set.<br>\n",
    "<br>\n",
    "\n",
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "- Fit a Perceptron model using your own selected feature columns.\n",
    "- Fine-tune the learning rate and epochs (max iterations). \n",
    "- Calculate the accuracy for both the train set predictions and test set predictions, what happens per iteration?\n",
    "- Is the accurracy different? Did you expect this difference?\n",
    "\n",
    "Optional: Perform the same tasks but change the hidden_layer_sizes <br>\n",
    "\n",
    "Findings: ...<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
